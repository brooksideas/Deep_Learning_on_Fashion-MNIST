{"cells":[{"source":"# Discussion - Module 4\n# Pros and Cons of Neural Network Optimization Algorithms","metadata":{},"cell_type":"markdown","id":"9b731c22-d944-4a34-b335-88e57f200e2c"},{"source":"**Directions:**\nIn the following discussion, you must select one of the following optimization algorithms: (1) Nesterov momentum, (2) RMSProp, (3) Adam, or (4) BFGS algorithm; try it on the machine learning dataset Fashion-MNIST; and respond to the following prompts:\n\n1. Report the validation error on Fashion-MNIST using your selected algorithm as the optimizer.   \n2. Compare the convergence speed of SGD against your selected algorithm. What improvements did your algorithm have over SGD?\n3. Experiment with different choices for adjusting the learning rate. Which learning rate returns the best validation error?\n4. What challenges involved in this deep optimization algorithm can you think of?","metadata":{},"cell_type":"markdown","id":"f7e9a47b-6c83-41b0-a2b7-e1bfb6ee1242"},{"source":"! pip install torch torchvision","metadata":{"executionCancelledAt":null,"executionTime":4698,"lastExecutedAt":1694560160630,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"! pip install torch torchvision","outputsMetadata":{"0":{"height":388,"type":"stream"}}},"cell_type":"code","id":"bc9cda08-d74a-4e6f-ba10-7e71bc60a9b6","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n"}]},{"source":"# Simple Python code snippet to train a neural network on the Fashion-MNIST dataset using the PyTorch framework and the Adam optimizer. ","metadata":{},"cell_type":"markdown","id":"fcb38d55-e253-477c-bb81-e1616263c4cf"},{"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\n\n# Set random seed for reproducibility\ntorch.manual_seed(3)\n\n# Define a simple neural network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Hyperparameters\nbatch_size = 64\nlearning_rate = 0.001  # Adjust the learning rates here\nepochs = 10\n\n# Load Fashion-MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n# Split the dataset into training and validation sets (e.g., 80% train, 20% validation)\ntrain_size = int(0.8 * len(trainset))\nval_size = len(trainset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n# Initialize the model and optimizer\nmodel = Net()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Use Adam optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with validation\nfor epoch in range(epochs):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    # Calculate validation loss\n    val_loss = 0.0\n    with torch.no_grad():\n        for data in valloader:\n            inputs, labels = data\n            outputs = model(inputs)\n            val_loss += criterion(outputs, labels).item()\n    \n    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(trainloader)}, Validation Loss: {val_loss / len(valloader)}\")\n\nprint(\"Finished Training\")\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":232,"type":"stream"},"2":{"height":115,"type":"stream"},"4":{"height":115,"type":"stream"},"6":{"height":115,"type":"stream"},"8":{"height":271,"type":"stream"}}},"cell_type":"code","id":"c77c49a3-dd3b-4adc-a349-066ade51bade","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1, Training Loss: 0.5430803653200468, Validation Loss: 0.4048712338380357\nEpoch 2, Training Loss: 0.3924556744496028, Validation Loss: 0.38634490190034215\nEpoch 3, Training Loss: 0.3522614185611407, Validation Loss: 0.342245960409971\nEpoch 4, Training Loss: 0.32539155301451683, Validation Loss: 0.34133574984809184\nEpoch 5, Training Loss: 0.3049181672135989, Validation Loss: 0.3376363071196891\nEpoch 6, Training Loss: 0.2906282551288605, Validation Loss: 0.3440606260394796\nEpoch 7, Training Loss: 0.27523438665270805, Validation Loss: 0.3186814149326466\nEpoch 8, Training Loss: 0.2636294405559699, Validation Loss: 0.3324200415468596\nEpoch 9, Training Loss: 0.253545482908686, Validation Loss: 0.32546375137060246\nEpoch 10, Training Loss: 0.24232167422771453, Validation Loss: 0.3303912768655635\nFinished Training\n"}]},{"source":"# Compare the convergence speed of SGD against your selected algorithm. What improvements did your algorithm have over SGD?","metadata":{},"cell_type":"markdown","id":"6bb46e89-25d1-415c-ad84-ffb03ec3528f"},{"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\n\n# Set random seed for reproducibility\ntorch.manual_seed(3)\n\n# Define a simple neural network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Hyperparameters\nbatch_size = 64\nlearning_rate = 0.001 # Adjust the learning rates here\nepochs = 10\n\n# Load Fashion-MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n# Split the dataset into training and validation sets (e.g., 80% train, 20% validation)\ntrain_size = int(0.8 * len(trainset))\nval_size = len(trainset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n# Initialize the model and optimizer for SGD\nmodel_sgd = Net()\noptimizer_sgd = optim.SGD(model_sgd.parameters(), lr=learning_rate)  # Use SGD optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with SGD\nfor epoch in range(epochs):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        optimizer_sgd.zero_grad()\n        outputs = model_sgd(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer_sgd.step()\n        running_loss += loss.item()\n    \n    # Calculate validation loss\n    val_loss = 0.0\n    with torch.no_grad():\n        for data in valloader:\n            inputs, labels = data\n            outputs = model_sgd(inputs)\n            val_loss += criterion(outputs, labels).item()\n    \n    print(f\"Epoch {epoch+1}, Training Loss (SGD): {running_loss / len(trainloader)}, Validation Loss: {val_loss / len(valloader)}\")\n\nprint(\"Finished Training with SGD\")","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":232,"type":"stream"}}},"cell_type":"code","id":"36cdacc1-ff77-49f5-9e21-77677c19e9df","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1, Training Loss (SGD): 2.2087842995325726, Validation Loss: 2.0642517513417182\nEpoch 2, Training Loss (SGD): 1.835270226319631, Validation Loss: 1.6002116894468348\nEpoch 3, Training Loss (SGD): 1.4162894668579102, Validation Loss: 1.2574609961915524\nEpoch 4, Training Loss (SGD): 1.1445761154492695, Validation Loss: 1.042324146691789\nEpoch 5, Training Loss (SGD): 0.9742124049663544, Validation Loss: 0.9061808991939464\nEpoch 6, Training Loss (SGD): 0.8652092943191528, Validation Loss: 0.8169343173503876\nEpoch 7, Training Loss (SGD): 0.7922242699464163, Validation Loss: 0.7563707964851502\nEpoch 8, Training Loss (SGD): 0.7414061694939931, Validation Loss: 0.7131430947400154\nEpoch 9, Training Loss (SGD): 0.7043368597428004, Validation Loss: 0.6826441643085885\nEpoch 10, Training Loss (SGD): 0.6761515806516012, Validation Loss: 0.6576209036593742\nFinished Training with SGD\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}